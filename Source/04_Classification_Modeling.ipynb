{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classifiers as clf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Species Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "species = pd.read_pickle(\"../Data/species.pkl\")\n",
    "species.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Target and Feature Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target and feature variables\n",
    "y = species['Federal Listing Status']\n",
    "X = species.drop(['Federal Listing Status', 'Scientific Name', 'Common Name'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "# federal listing status\n",
    "print('Training Data: ' + str(y_train.count()))\n",
    "print('Endangered:    ' + str((y_train == 'Endangered').sum() / y_train.count()))\n",
    "print('Not Listed:    ' + str((y_train == 'Not Listed').sum() / y_train.count()))\n",
    "print('Threatened:    ' + str((y_train == 'Threatened').sum() / y_train.count()))\n",
    "\n",
    "# federal listing status\n",
    "print('Test Data:     ' + str(y_test.count()))\n",
    "print('Endangered:    ' + str((y_test == 'Endangered').sum() / y_test.count()))\n",
    "print('Not Listed:    ' + str((y_test == 'Not Listed').sum() / y_test.count()))\n",
    "print('Threatened:    ' + str((y_test == 'Threatened').sum() / y_test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Classification Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for all classifiers\n",
    "classifiers = []\n",
    "\n",
    "# dummy classifier\n",
    "classifiers.append({'Model': 'Dummy',\n",
    "                    'Classifier': DummyClassifier(strategy='most_frequent')})\n",
    "\n",
    "# logistic regression classifer\n",
    "classifiers.append({'Model': 'Logistic Regression',\n",
    "                    'Classifier': LogisticRegression(C=1e12,\n",
    "                                                     penalty='l1',\n",
    "                                                     multi_class='ovr',\n",
    "                                                     solver='liblinear',\n",
    "                                                     n_jobs=-1)})\n",
    "\n",
    "# k nearest neighbors classifer\n",
    "classifiers.append({'Model': 'K Nearest Neighbors',\n",
    "                    'Classifier': KNeighborsClassifier(n_neighbors=5)})\n",
    "\n",
    "# decision tree classifer\n",
    "classifiers.append({'Model': 'Decision Tree',\n",
    "                    'Classifier': DecisionTreeClassifier(criterion='gini',\n",
    "                                                         max_depth=5,\n",
    "                                                         max_features=0.8)})\n",
    "\n",
    "# random forest classifer\n",
    "classifiers.append({'Model': 'Random Forest',\n",
    "                    'Classifier': RandomForestClassifier(n_estimators=100,\n",
    "                                                         criterion='gini',\n",
    "                                                         max_depth=5,\n",
    "                                                         max_features=0.8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifiers, metrics = clf.classify(classifiers, X_train, X_test, y_train, y_test)\n",
    "\n",
    "baseline = pd.DataFrame(metrics, columns=['Model',\n",
    "                                          'Split',\n",
    "                                          'Accuracy',\n",
    "                                          'Precision',\n",
    "                                          'Recall',\n",
    "                                          'F1 Score'])\n",
    "baseline['Configuration'] = 'Baseline'\n",
    "models = baseline.copy()\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline.pivot(index='Model', columns='Split').drop(\n",
    "    'Configuration', axis=1).sort_values(by=('F1 Score','Train'), ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Class Imbalance with SMOTE Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# federal listing status\n",
    "print(y_train.count())\n",
    "print('Endangered: ' + str((y_train == 'Endangered').sum() / y_train.count()))\n",
    "print('Not Listed: ' + str((y_train == 'Not Listed').sum() / y_train.count()))\n",
    "print('Threatened: ' + str((y_train == 'Threatened').sum() / y_train.count()))\n",
    "\n",
    "# plot class imbalance\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "sns.countplot(y_train, alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(y_train.unique())\n",
    "values = np.ones((3), dtype=int) * (y_train == 'Not Listed').sum()\n",
    "sample_ratio = dict(zip(keys, values))\n",
    "sample_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species['Federal Listing Status'].unique()\n",
    "\n",
    "smote = SMOTE(ratio=sample_ratio)\n",
    "X_train_smote, y_train_smote = smote.fit_sample(X_train, y_train)\n",
    "\n",
    "# federal listing status\n",
    "print(len(y_train_smote))\n",
    "print('Endangered: ' + str((y_train_smote == 'Endangered').sum() / len(y_train_smote)))\n",
    "print('Not Listed: ' + str((y_train_smote == 'Not Listed').sum() / len(y_train_smote)))\n",
    "print('Threatened: ' + str((y_train_smote == 'Threatened').sum() / len(y_train_smote)))\n",
    "\n",
    "# plot class imbalance\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "sns.countplot(y_train_smote, alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers, metrics = clf.classify(classifiers, X_train_smote, X_test, y_train_smote, y_test)\n",
    "\n",
    "balanced = pd.DataFrame(metrics, columns=['Model',\n",
    "                                          'Split',\n",
    "                                          'Accuracy',\n",
    "                                          'Precision',\n",
    "                                          'Recall',\n",
    "                                          'F1 Score'])\n",
    "\n",
    "balanced['Configuration'] = 'Balanced'\n",
    "models = models.append(balanced, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.sort_values(by='F1 Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "balanced.pivot(index='Model', columns='Split').drop(\n",
    "    'Configuration', axis=1).sort_values(by=('F1 Score','Train'), ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned and Balanced Grid Search Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for all grid searches\n",
    "grid_searches = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression parameters\n",
    "parameters_log = dict(penalty=['l1', 'l2'],\n",
    "                      C=[1e-2, 1e0, 1e2, 1e6, 1e12],\n",
    "                      fit_intercept=[True, False],\n",
    "                      multi_class=['ovr'],\n",
    "                      solver=['liblinear'])\n",
    "\n",
    "grid_searches.append({'Model': 'Logistic Regression',\n",
    "                    'Classifier': GridSearchCV(LogisticRegression(),\n",
    "                                               parameters_log,\n",
    "                                               cv=5,\n",
    "                                               scoring='f1_weighted',\n",
    "                                               verbose=10,\n",
    "                                               n_jobs=-1)})\n",
    "\n",
    "# k nearest neighbors parameters\n",
    "parameters_knn = dict(n_neighbors=range(5,13,4),\n",
    "                      weights=['uniform', 'distance'],\n",
    "                      algorithm=['ball_tree','kd_tree'],\n",
    "                      leaf_size=range(10,20,10))\n",
    "\n",
    "grid_searches.append({'Model': 'K Nearest Neighbors',\n",
    "                    'Classifier': GridSearchCV(KNeighborsClassifier(),\n",
    "                                               parameters_knn,\n",
    "                                               cv=5,\n",
    "                                               scoring='f1_weighted',\n",
    "                                               verbose=10,\n",
    "                                               n_jobs=-1)})\n",
    "\n",
    "# decision tree parameters\n",
    "parameters_tree = dict(criterion=['gini','entropy'],\n",
    "                       max_depth=range(2,5),\n",
    "                       min_samples_leaf=range(10,50,20),\n",
    "                       max_features=range(10,50,20),\n",
    "                       min_impurity_decrease=[0.01,0.03,0.05])\n",
    "\n",
    "grid_searches.append({'Model': 'Decision Tree',\n",
    "                    'Classifier': GridSearchCV(DecisionTreeClassifier(),\n",
    "                                               parameters_tree,\n",
    "                                               cv=5,\n",
    "                                               scoring='f1_weighted',\n",
    "                                               verbose=10,\n",
    "                                               n_jobs=-1)})\n",
    "\n",
    "# random forest parameters\n",
    "parameters_forest = dict(n_estimators=range(100,500,100),\n",
    "                         max_depth=range(2,5),\n",
    "                         min_samples_leaf=range(10,50,20),\n",
    "                         max_features=range(10,50,20),\n",
    "                         min_impurity_decrease=[0.01,0.03,0.05])\n",
    "\n",
    "grid_searches.append({'Model': 'Random Forest',\n",
    "                    'Classifier': GridSearchCV(RandomForestClassifier(),\n",
    "                                               parameters_forest,\n",
    "                                               cv=5,\n",
    "                                               scoring='f1_weighted',\n",
    "                                               verbose=10,\n",
    "                                               n_jobs=-1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_searches, metrics = clf.grid_search(grid_searches, X_train_smote, X_test, y_train_smote, y_test, X.columns)\n",
    "\n",
    "tuned = pd.DataFrame(metrics, columns=['Model',\n",
    "                                       'Split',\n",
    "                                       'Accuracy',\n",
    "                                       'Precision',\n",
    "                                       'Recall',\n",
    "                                       'F1 Score'])\n",
    "\n",
    "tuned['Configuration'] = 'Balanced and Tuned'\n",
    "models = models.append(tuned, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models.sort_values(by='F1 Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned.pivot(index='Model', columns='Split').drop(\n",
    "    'Configuration', axis=1).sort_values(by=('F1 Score','Train'), ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.to_pickle(\"../Data/models.pkl\")\n",
    "baseline.to_pickle(\"../Data/baseline.pkl\")\n",
    "balanced.to_pickle(\"../Data/balanced.pkl\")\n",
    "tuned.to_pickle(\"../Data/tuned.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
